# ğŸ©º Diabetes Prediction Project

## ğŸ“Œ Project Overview
This project predicts whether a person is **Diabetic or Non-Diabetic** based on key health features.  
We experimented with **Logistic Regression** first and then built a **Random Forest Classifier** for higher accuracy and robustness.

---

## ğŸ§© Dataset Details
- **Source:** Kaggle Dataset
- **Size:** 2768 samples  
- **Top Features Used:**
  1. **Glucose** ğŸ­  
  2. **BMI** âš–ï¸  
  3. **Age** ğŸ‘µğŸ‘´  
  4. **DiabetesPedigreeFunction** ğŸ§¬  
  5. **Pregnancies** ğŸ¤°
  6. **SkinThickness** ğŸ§´
  7. **Insulin** ğŸ’‰
  8. **BloodPressure** ğŸ©¸

- **Class Distribution:**  
  - Non-Diabetic: 65%  
  - Diabetic: 35%  
> âš ï¸ Dataset slightly imbalanced â†’ handled with class weighting and Random Forest.

---

## ğŸ”¹ Logistic Regression (Initial Attempt)
- Tried **Logistic Regression** first as a baseline model  
- **Hyperparameters tuned** using GridSearchCV:
  1. `penalty`: ['l1','l2']
  2. `C`: [0.01, 0.1, 1, 10, 100]
  3. `solver`: ['liblinear', 'saga']  

**Performance Metrics:**

| Class | Precision | Recall | F1-score | Support |
|-------|-----------|--------|----------|---------|
| 0     | 0.85      | 0.78   | 0.81     | 363     |
| 1     | 0.64      | 0.74   | 0.69     | 191     |
| **Accuracy** | â€” | â€” | 0.76     | 554     |

**Observations:**  
- Non-Diabetic class predicted well âœ…  
- Diabetic class (minority) had lower precision âš ï¸  
- Decided to use **Random Forest** for better handling of class imbalance and higher overall accuracy  

---

## ğŸ›  Random Forest Classifier

### 1ï¸âƒ£ Feature Selection
- Based on feature importance from RF, skipped low-importance features:
  - Removed: Insulin, SkinThickness, BloodPressure  
  - Kept: Glucose, BMI, Age, DiabetesPedigreeFunction, Pregnancies âœ…

### 2ï¸âƒ£ Train-Test Split
- Stratified split to maintain class balance  
- Train:Test = 70:30  

### 3ï¸âƒ£ Model Training & Hyperparameter Tuning
- Used GridSearchCV to tune:
  - n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, class_weight
- `Accuracy` used as a scoring metric

### 4ï¸âƒ£ Model Evaluation
Confusion matrix:
| Actual/Predicted | Non-Diabetic (0) | Diabetic (1) |
|-------|-----------|--------|
| Non-Diabetic (0) | 537 | 8 |
| Diabetic (1) | 7 | 277 |

Classification metric:
| Class | Precision | Recall | F1-score |
|-------|-----------|--------|----------|
| 0     | 0.98     | 0.99  | 0.98    | 
| 1     | 0.97     | 0.97  | 0.97     |
| **Accuracy** | â€” | â€” | 0.98    |

Cross-validation score: 0.99
âœ… Model generalizes well and predicts both classes accurately.

### 5ï¸âƒ£ Feature Importance (Top 5)
| Feature | Importance |
|--------|----------|
| Glucose |	0.3299 ğŸ­ |
| BMI |	0.2297 âš–ï¸ |
| Age	| 0.1837 ğŸ‘µğŸ‘´ |
| DiabetesPedigreeFunction |	0.1526 ğŸ§¬ |
| Pregnancies | 0.1041 ğŸ¤° |
- Glucose and BMI are the most influential features
- Low-importance features were removed â†’ model simpler and cleaner

## ğŸš€ Next Steps / Deployment
1. Deploy as web or mobile app
2. Predict new patient data (top 5 features)
3. Optionally adjust probability thresholds for higher recall of diabetic class
4. Explain predictions using SHAP/LIME for transparency

## âš¡ Summary
1. Logistic Regression: baseline, Accuracy = 0.76
2. Random Forest: final model, Accuracy = 0.98, CV = 0.99
3. Top 5 features identified and visualized
4. Model saved and ready for deployment

## ğŸ‘¨â€ğŸ’» Author
**Aldous Dsouza**  
- BSc Computer Science Graduate  
- Interested in Data Science, Machine Learning, and Software Development    

